[hyperparameters]
epochs=100
batch_size=256
embedding_size=16
layers=[64,32,16,8]
reg_mf=0
reg_mlp=0
ispairwise=false
num_neg=4
#pairwise:BPR,hinge,square
#pointwise:cross_entropy,square
loss_function=cross_entropy
topK=10
learning_rate=0.01
learner=adam
verbose=1
mf_pretrain= save/GMF
mlp_pretrain= save/MLP
out= true
pretrain_epochs=0
